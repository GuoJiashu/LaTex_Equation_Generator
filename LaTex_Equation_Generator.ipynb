{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5c004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import Levenshtein\n",
    "import torch\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet34\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from matplotlib import gridspec\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image, ImageOps\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a465e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data directory and file paths\n",
    "data_dir = r\"C:\\Users\\13658\\Desktop\\LaTex_Code_Generator\"\n",
    "handwritten_equations = os.path.join(data_dir, \"Handwritten_equations\")\n",
    "json_dict = os.path.join(data_dir, \"char_dict.json\")\n",
    "csv_file = os.path.join(data_dir, \"caption_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e51cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and merge spaces in LaTeX code intelligently\n",
    "def smart_clean_latex(code):\n",
    "    preserved = re.findall(r'(\\\\[a-zA-Z]+)\\s+', code)\n",
    "    for cmd in preserved:\n",
    "        code = code.replace(cmd + ' ', f'{cmd}<<<SPACE>>>')\n",
    "    code = code.replace(' ', '')\n",
    "    code = code.replace('<<<SPACE>>>', ' ')\n",
    "    return code\n",
    "\n",
    "# Load the CSV file and process LaTeX codes\n",
    "csv_data = pd.read_csv(csv_file)\n",
    "latex_codes_raw = csv_data['Column2'].values.tolist()\n",
    "latex_codes = [smart_clean_latex(str(code)) for code in latex_codes_raw]\n",
    "image_names = csv_data['Column1'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaa51fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all LaTeX command tokens from the cleaned LaTeX codes\n",
    "command_tokens = set()\n",
    "for code in latex_codes:\n",
    "    i = 0\n",
    "    while i < len(code):\n",
    "        if code[i] == '\\\\':\n",
    "            j = i + 1\n",
    "            while j < len(code) and code[j].isalpha():\n",
    "                j += 1\n",
    "            command_tokens.add(code[i:j])\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "# Tokenize LaTeX code into a mix of full commands and individual characters\n",
    "def mixed_tokenize_latex(code, command_set):\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(code):\n",
    "        if code[i] == '\\\\':\n",
    "            j = i + 1\n",
    "            while j < len(code) and code[j].isalpha():\n",
    "                j += 1\n",
    "            cmd = code[i:j]\n",
    "            if cmd in command_set:\n",
    "                tokens.append(cmd)\n",
    "                i = j\n",
    "                continue\n",
    "        tokens.append(code[i])\n",
    "        i += 1\n",
    "    return tokens\n",
    "\n",
    "# Count token frequencies across all LaTeX codes\n",
    "token_counter = Counter()\n",
    "for code in latex_codes:\n",
    "    tokens = mixed_tokenize_latex(code, command_tokens)\n",
    "    token_counter.update(tokens)\n",
    "\n",
    "# Define special tokens and build token-index dictionaries\n",
    "special_tokens = ['<pad>', '<s>', '</s>']\n",
    "token_list = special_tokens + sorted(token_counter.keys())\n",
    "\n",
    "token2idx = {token: idx for idx, token in enumerate(token_list)}\n",
    "idx2token = {idx: token for token, idx in token2idx.items()}\n",
    "vocab_size = len(token2idx)\n",
    "\n",
    "# Encode LaTeX code into a sequence of token indices\n",
    "def encode_label_mixed(code, token2idx, command_set, max_len):\n",
    "    tokens = ['<s>'] + mixed_tokenize_latex(code, command_set)[:max_len - 2] + ['</s>']\n",
    "    label = np.zeros(max_len, dtype=np.int32)\n",
    "    for t, token in enumerate(tokens):\n",
    "        label[t] = token2idx.get(token, 0)\n",
    "    return label\n",
    "\n",
    "max_seq_length = max(len(mixed_tokenize_latex(code, command_tokens)) for code in latex_codes) + 2\n",
    "print(f\"Dic Length: {vocab_size}\")\n",
    "print(f\"Maximum Sequence: {max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a2a296",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.RandomRotation(degrees=2),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "def apply_augmentation(image_tensor):\n",
    "    return train_transform(image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cf3dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess a single image: grayscale, resize with padding, normalize, and expand to 3 channels\n",
    "def preprocess_image(image_path, target_size=(512, 128)):\n",
    "    img = Image.open(image_path).convert('L')\n",
    "    img.thumbnail(target_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "    delta_w = target_size[0] - img.size[0]\n",
    "    delta_h = target_size[1] - img.size[1]\n",
    "    padding = (delta_w // 2, delta_h // 2, delta_w - delta_w // 2, delta_h - delta_h // 2)\n",
    "    img = ImageOps.expand(img, padding, fill=255)\n",
    "\n",
    "    img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = np.repeat(img_array, 3, axis=0)\n",
    "    return torch.from_numpy(img_array).float()\n",
    "\n",
    "# Define a custom dataset for LaTeX images and labels\n",
    "class LatexDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_names, latex_codes, max_len, augment=False):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_names = image_names\n",
    "        self.latex_codes = latex_codes\n",
    "        self.max_len = max_len\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, f\"{self.image_names[idx]}.bmp\")\n",
    "        img = preprocess_image(img_path)\n",
    "\n",
    "        if self.augment:\n",
    "            img = apply_augmentation(img)\n",
    "\n",
    "        label = encode_label_mixed(self.latex_codes[idx], token2idx, command_tokens, self.max_len)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        return img, label_tensor\n",
    "\n",
    "# Define a custom collate function to batch images and pad label sequences\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=0)\n",
    "    return images, labels, [len(l) for l in labels]\n",
    "\n",
    "# Split the dataset into training, validation, and test sets (70% / 15% / 15%)\n",
    "train_imgs, tep_imgs, train_codes, tep_codes = train_test_split(\n",
    "    image_names, latex_codes, test_size=0.3, random_state=42)\n",
    "\n",
    "val_imgs, test_imgs, val_codes, test_codes = train_test_split(\n",
    "    tep_imgs, tep_codes, test_size=0.5, random_state=41)\n",
    "\n",
    "# Splited dataset statistics\n",
    "train_dataset = LatexDataset(handwritten_equations, train_imgs, train_codes, max_seq_length, augment=True)\n",
    "val_dataset = LatexDataset(handwritten_equations, val_imgs, val_codes, max_seq_length, augment=False)\n",
    "test_dataset = LatexDataset(handwritten_equations, test_imgs, test_codes, max_seq_length, augment=False)\n",
    "\n",
    "# Data loaders for training, validation, and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fec02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for saving token dictionaries\n",
    "dict_dir = os.path.join(data_dir, \"token_dicts\")\n",
    "os.makedirs(dict_dir, exist_ok=True)\n",
    "\n",
    "# Save token2idx and idx2token mappings as pickle files\n",
    "with open(os.path.join(dict_dir, \"token2idx.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(token2idx, f)\n",
    "\n",
    "with open(os.path.join(dict_dir, \"idx2token.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(idx2token, f)\n",
    "\n",
    "# Plot the histogram of LaTeX sequence lengths\n",
    "seq_lens = [len(mixed_tokenize_latex(code, command_tokens)) for code in latex_codes]\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(seq_lens, bins=40, color='skyblue', edgecolor='black')\n",
    "plt.axvline(max_seq_length - 2, color='red', linestyle='--', label='max_seq_length (without <s>, </s>)')\n",
    "plt.title(\"LaTeX Sequence Length Distribution\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Sample Count\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94ebac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a random preprosessed image and its corresponding LaTeX code\n",
    "idx = random.randint(0, len(train_dataset) - 1)\n",
    "img_tensor, label_tensor = train_dataset[idx]\n",
    "\n",
    "print(f\"Figure shape: {img_tensor.shape}\")\n",
    "print(f\"Label shape: {label_tensor.shape}\")\n",
    "\n",
    "plt.imshow(img_tensor.permute(1, 2, 0).numpy(), cmap='gray')\n",
    "plt.title(\"Preprocessed Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fc9a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode and display the LaTeX expression for the selected image\n",
    "sample_indices = random.sample(range(len(train_dataset)), 5)\n",
    "\n",
    "for i, idx in enumerate(sample_indices, 1):\n",
    "    img_tensor, label_tensor = train_dataset[idx]\n",
    "    \n",
    "    tokens = [idx2token[idx.item()] for idx in label_tensor \n",
    "              if idx.item() not in (token2idx['<pad>'], token2idx['<s>'], token2idx['</s>'])]\n",
    "    decoded_latex = ' '.join(tokens)\n",
    "\n",
    "    print(f\"\\nSample {i}\")\n",
    "    print(\"Token Length:\", len(tokens))\n",
    "    print(\"Restore the LaTeX expression:\")\n",
    "    print(decoded_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dadb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode and display a random LaTeX code from the dataset\n",
    "sample_idx = random.randint(0, len(latex_codes_raw) - 1)\n",
    "raw = latex_codes_raw[sample_idx]\n",
    "cleaned = smart_clean_latex(raw)\n",
    "encoded = encode_label_mixed(cleaned, token2idx, command_tokens, max_seq_length)\n",
    "\n",
    "tokens = [idx2token[i] for i in encoded if i not in (token2idx['<pad>'], token2idx['<s>'], token2idx['</s>'])]\n",
    "decoded = ''.join(tokens)\n",
    "\n",
    "print(\"Original Lable:\")\n",
    "print(raw)\n",
    "print(\"\\nWashed:\")\n",
    "print(cleaned)\n",
    "print(\"\\nEncoded:\")\n",
    "print(encoded)\n",
    "print(\"\\nDecoded:\")\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac077749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes and label lengths for inspection\n",
    "batch = next(iter(train_loader))\n",
    "imgs, labels, lengths = batch\n",
    "\n",
    "print(f\"Batch figure shape: {imgs.shape}\")\n",
    "print(f\"Batch label shape: {labels.shape}\")\n",
    "print(f\"Label Length (first 5): {lengths[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaead69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 2D positional encoding for CNN feature maps\n",
    "class PositionalEncoding2D(nn.Module):\n",
    "    def __init__(self, d_model, height, width):\n",
    "        super().__init__()\n",
    "        if d_model % 4 != 0:\n",
    "            raise ValueError(\"d_model must be divisible by 4\")\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        pe = torch.zeros(d_model, height, width) # Initialize a zero tensor for positional encoding\n",
    "\n",
    "        d_model_half = d_model // 2\n",
    "        div_term = torch.exp(torch.arange(0., d_model_half, 2) * -(math.log(10000.0) / d_model_half))\n",
    "\n",
    "        pos_w = torch.arange(0., width).unsqueeze(1)\n",
    "        pos_h = torch.arange(0., height).unsqueeze(1)\n",
    "\n",
    "        # Horizontal coding (width direction)\n",
    "        pe[0:d_model_half:2, :, :] = torch.sin(pos_w * div_term).T.unsqueeze(1).repeat(1, height, 1)\n",
    "        pe[1:d_model_half:2, :, :] = torch.cos(pos_w * div_term).T.unsqueeze(1).repeat(1, height, 1)\n",
    "\n",
    "        # Vertical encoding (height direction)\n",
    "        pe[d_model_half::2, :, :] = torch.sin(pos_h * div_term).T.unsqueeze(2).repeat(1, 1, width)\n",
    "        pe[d_model_half+1::2, :, :] = torch.cos(pos_h * div_term).T.unsqueeze(2).repeat(1, 1, width)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :, :x.size(2), :x.size(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Encoder class using ResNet34 as the backbone\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model=384):\n",
    "        super().__init__()\n",
    "        base_cnn = resnet34(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(base_cnn.children())[:6])\n",
    "\n",
    "        self.project = nn.Conv2d(128, d_model, kernel_size=1)\n",
    "        self.row_encoder = nn.LSTM(input_size=d_model,\n",
    "                                   hidden_size=d_model // 2,\n",
    "                                   batch_first=True,\n",
    "                                   bidirectional=True)\n",
    "\n",
    "        self.pe = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.backbone(x)\n",
    "        feat = self.project(feat)\n",
    "        B, C, H, W = feat.shape\n",
    "\n",
    "        if self.pe is None:\n",
    "            self.pe = PositionalEncoding2D(C, H, W).to(x.device)\n",
    "        feat = self.pe(feat)\n",
    "\n",
    "        # Run Row-wise LSTM\n",
    "        feat = feat.permute(0, 2, 3, 1).contiguous()\n",
    "        feat = feat.view(B * H, W, C)\n",
    "        feat, _ = self.row_encoder(feat)\n",
    "        feat = feat.view(B, H, W, C)\n",
    "\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee69737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run train loader to get a batch of images\n",
    "images, _, _ = next(iter(train_loader))\n",
    "images = images.cuda()\n",
    "\n",
    "# Encoder initialization\n",
    "encoder = Encoder(d_model=256).cuda()\n",
    "\n",
    "# Forward Pass\n",
    "features = encoder(images)\n",
    "\n",
    "print(\"Encoder Output:\", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0b7467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the images on different layers of the encoder\n",
    "class VisualCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        base = resnet34(pretrained=True)\n",
    "        self.stages = nn.ModuleList([\n",
    "            nn.Sequential(base.conv1, base.bn1, base.relu),  # stage 0\n",
    "            base.maxpool,                                    # stage 1\n",
    "            base.layer1,                                     # stage 2\n",
    "            base.layer2,                                     # stage 3\n",
    "            base.layer3                                      # stage 4\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for stage in self.stages:\n",
    "            x = stage(x)\n",
    "            outputs.append(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cbe15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the feature maps from the CNN\n",
    "def show_feature_map(tensor, title, max_channels=4):\n",
    "    n = min(max_channels, tensor.shape[1])  # Show first n channels\n",
    "    fig, axes = plt.subplots(1, n, figsize=(3 * n, 3))\n",
    "    for i in range(n):\n",
    "        axes[i].imshow(tensor[0, i].detach().cpu(), cmap='gray')\n",
    "        axes[i].set_title(f\"{title} | Channel {i}\")\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a1db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of images from the train loader\n",
    "images, _, _ = next(iter(train_loader))\n",
    "sample_img = images[0:1].cuda()\n",
    "\n",
    "# Instantiate the VisualCNN and get feature maps\n",
    "viz_cnn = VisualCNN().cuda()\n",
    "features = viz_cnn(sample_img)\n",
    "\n",
    "# Show every stage's feature map\n",
    "for idx, feat in enumerate(features):\n",
    "    print(f\"Stage {idx} output shape: {feat.shape}\")\n",
    "    show_feature_map(feat, f\"Stage {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3490e0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the maximum activation of the feature map\n",
    "def show_max_activation(feature_map, title=\"Max Activation\"):\n",
    "    \n",
    "    max_activations = torch.max(feature_map[0], dim=0).values.detach().cpu()\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(max_activations, cmap='hot')\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cde506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the maximum activation of each stage's feature map\n",
    "for idx, feat in enumerate(features):\n",
    "    print(f\"Stage {idx} output shape: {feat.shape}\")\n",
    "    show_max_activation(feat, f\"Stage {idx} Max Activation Heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6011c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Decoder class using Transformer Decoder layers\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=384, nhead=8, num_layers=8, max_len=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Token embedding layer (maps token indices to vectors)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "         # Learnable positional embeddings\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, max_len * 2, d_model))\n",
    "\n",
    "         # Learnable positional embeddings\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        nn.init.normal_(self.pos_embedding, std=0.02)\n",
    "\n",
    "    def forward(self, tgt_input, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None):\n",
    "\n",
    "        tgt_emb = self.token_embedding(tgt_input) + self.pos_embedding[:, :tgt_input.size(1), :]\n",
    "\n",
    "        memory = memory.reshape(memory.size(0), -1, memory.size(-1))\n",
    "\n",
    "        output = self.transformer_decoder(\n",
    "            tgt=tgt_emb,\n",
    "            memory=memory,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "        return self.output_layer(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e9237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a square mask for sequence-to-sequence decoding\n",
    "# Masked positions are filled with -inf, non-masked with 0.0\n",
    "def generate_square_subsequent_mask(sz: int) -> torch.Tensor:\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07954615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pack the Encoder and Decoder into a single model\n",
    "class Im2LatexModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=384, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(d_model=d_model)\n",
    "        self.decoder = Decoder(vocab_size=vocab_size, d_model=d_model, max_len=max_seq_len)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def forward(self, images, tgt_input, tgt_mask=None):\n",
    "        enc_output = self.encoder(images)  # [B, H, W, D]\n",
    "        logits = self.decoder(tgt_input, enc_output, tgt_mask=tgt_mask)\n",
    "        return logits  # [B, T, vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e18ab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teset the model with a batch of images and labels\n",
    "images, labels, _ = next(iter(train_loader))\n",
    "images = images.cuda()\n",
    "labels = labels.cuda()\n",
    "\n",
    "# Create input by shifting\n",
    "tgt_input = labels[:, :-1]\n",
    "tgt_output = labels[:, 1:]\n",
    "\n",
    "model = Im2LatexModel(vocab_size=len(token2idx), d_model=256, max_seq_len=max_seq_length).cuda()\n",
    "tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(images.device)\n",
    "\n",
    "logits = model(images, tgt_input, tgt_mask)  # [B, T, vocab_size]\n",
    "\n",
    "print(\"Decoder output:\", logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5e6229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the integrity of the decoder\n",
    "def test_decoder_integrity(model, images, labels, token2idx, idx2token):\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Running Decoder Integrity Tests\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    tgt_input = labels[:, :-1]\n",
    "    tgt_output = labels[:, 1:]\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
    "\n",
    "    # Forward\n",
    "    logits = model(images, tgt_input, tgt_mask)\n",
    "\n",
    "    # Test 1 - Logits stats\n",
    "    print(\"\\n[1] Logits distribution:\")\n",
    "    print(\"   mean:\", logits.mean().item())\n",
    "    print(\"   max :\", logits.max().item())\n",
    "    print(\"   min :\", logits.min().item())\n",
    "\n",
    "    # Test 2 - Sample Top-K token at t=10\n",
    "    t = min(10, logits.size(1) - 1)\n",
    "    sample_logits = logits[0, t]\n",
    "    probs = torch.softmax(sample_logits, dim=-1)\n",
    "    topk_probs, topk_idx = torch.topk(probs, k=5)\n",
    "\n",
    "    print(\"\\n[2] Top-5 tokens at timestep\", t)\n",
    "    for i in range(5):\n",
    "        print(f\"   {idx2token[topk_idx[i].item()]}: {topk_probs[i].item():.4f}\")\n",
    "\n",
    "    # Test 3 - First-step decoder only\n",
    "    print(\"\\n[3] First-step decoding test\")\n",
    "    start = torch.full((1, 1), token2idx['<s>'], dtype=torch.long).to(device)\n",
    "    enc_out = model.encoder(images[0:1])\n",
    "    logits_start = model.decoder(start, enc_out)\n",
    "    pred = torch.argmax(logits_start, dim=-1)[0, 0].item()\n",
    "    print(f\"   Predicted first token: {idx2token[pred]}\")\n",
    "\n",
    "    # Test 4 - Gradient check\n",
    "    print(\"\\n[4] Gradient flow check\")\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    labels_flat = tgt_output.contiguous().view(-1)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=token2idx['<pad>'])\n",
    "    loss = loss_fn(logits_flat, labels_flat)\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            print(f\"   {name:40s} | grad norm = {param.grad.norm():.4f}\")\n",
    "\n",
    "    print(\"\\nDecoder passed all integrity checks (forward & backward).\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da8c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the decoder integrity with a batch of images and labels\n",
    "images, labels, _ = next(iter(train_loader))\n",
    "test_decoder_integrity(model, images, labels, token2idx, idx2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8930824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for training (label smoothing)\n",
    "def create_loss_function(token2idx, smoothing=0.1):\n",
    "    ignore_index = token2idx['<pad>']\n",
    "    return nn.CrossEntropyLoss(ignore_index=ignore_index, label_smoothing=smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfdb2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the one-epoch training function\n",
    "def train_one_epoch(model, dataloader, optimizer, scheduler, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels, _ in tqdm(dataloader, desc=\"Training\"):\n",
    "        # Use GPU if available\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Prepare target input (for prediction) and target output (for supervision)\n",
    "        tgt_input = labels[:, :-1]\n",
    "        tgt_output = labels[:, 1:]\n",
    "\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images, tgt_input, tgt_mask)\n",
    "\n",
    "        # Reshape logits and targets for loss computation\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        tgt_output = tgt_output.contiguous().view(-1)\n",
    "\n",
    "         # Compute loss and backpropagate\n",
    "        loss = loss_fn(logits, tgt_output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d548e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation function (no gradient calculation)\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels, _ in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        tgt_input = labels[:, :-1]\n",
    "        tgt_output = labels[:, 1:]\n",
    "\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
    "\n",
    "        logits = model(images, tgt_input, tgt_mask)\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        tgt_output = tgt_output.contiguous().view(-1)\n",
    "\n",
    "        loss = loss_fn(logits, tgt_output)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea62373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a learning rate scheduler with linear warmup and cosine annealing\n",
    "def get_scheduler(optimizer, num_warmup_steps, num_training_steps):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        return max(\n",
    "            0.0, \n",
    "            0.5 * (1.0 + math.cos(math.pi * (current_step - num_warmup_steps) / (num_training_steps - num_warmup_steps)))\n",
    "        )\n",
    "    \n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe5b008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the full training loop with validation and checkpoint saving\n",
    "def fit(model, train_loader, val_loader, token2idx, epochs=10, lr=1e-4, device=\"cuda\"):\n",
    "    # Setup optimizer, loss function, scheduler\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = create_loss_function(token2idx, smoothing=0.1)\n",
    "\n",
    "    num_training_steps = epochs * len(train_loader)\n",
    "    num_warmup_steps = int(0.1 * num_training_steps)  # warmup 10% step\n",
    "    scheduler = get_scheduler(optimizer, num_warmup_steps, num_training_steps)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    save_path = \"best_model.pt\"\n",
    "\n",
    "    writer = SummaryWriter(log_dir=\"runs/im2latex\")\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Measure initial loss before training\n",
    "    print(\"\\nMeasuring Initial Loss...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(val_loader))\n",
    "        images, labels, _ = batch\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        tgt_input = labels[:, :-1]\n",
    "        tgt_output = labels[:, 1:]\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
    "\n",
    "        logits = model(images, tgt_input, tgt_mask)\n",
    "        loss = loss_fn(logits.reshape(-1, logits.size(-1)), tgt_output.reshape(-1))\n",
    "        print(f\"Initial Loss (on val batch): {loss.item():.4f}\")\n",
    "\n",
    "     # Start training loop\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, scheduler, loss_fn, device)\n",
    "        val_loss = evaluate(model, val_loader, loss_fn, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Saved best model to {save_path}\")\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    # Plot loss curve\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"loss_curve.png\")\n",
    "    plt.show()\n",
    "    print(\"Saved loss curve to loss_curve.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760b3473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training the model\n",
    "model = Im2LatexModel(\n",
    "    vocab_size=len(token2idx),\n",
    "    d_model=384,\n",
    "    max_seq_len=max_seq_length\n",
    ").to(\"cuda\")\n",
    "\n",
    "fit(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    token2idx=token2idx,\n",
    "    epochs=50,\n",
    "    lr=1e-4,\n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ab4f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode(model, image, token2idx, idx2token, beam_width=3, max_len=256, device=\"cuda\", length_penalty_alpha=0.6):\n",
    "    model.eval()\n",
    "\n",
    "    if not image.is_cuda:\n",
    "        image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        memory = model.encoder(image.unsqueeze(0))\n",
    "        memory = memory.expand(beam_width, *memory.shape[1:])\n",
    "\n",
    "        sequences = [[token2idx['<s>']] for _ in range(beam_width)]\n",
    "        scores = torch.zeros(beam_width, device=device)\n",
    "        completed = []\n",
    "\n",
    "        for step in range(max_len):\n",
    "            all_candidates = []\n",
    "            for i, seq in enumerate(sequences):\n",
    "                if len(seq) > 1 and seq[-1] == token2idx['</s>']:\n",
    "                    length_penalty = (len(seq)) ** length_penalty_alpha\n",
    "                    completed.append((seq, scores[i].item() / length_penalty))\n",
    "                    continue\n",
    "\n",
    "                tgt_input = torch.tensor(seq, dtype=torch.long, device=device).unsqueeze(0)\n",
    "                tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
    "                output = model.decoder(tgt_input, memory[i:i+1], tgt_mask)\n",
    "                probs = torch.log_softmax(output[0, -1], dim=-1)\n",
    "\n",
    "                topk_probs, topk_idx = torch.topk(probs, beam_width)\n",
    "                for j in range(beam_width):\n",
    "                    candidate = seq + [topk_idx[j].item()]\n",
    "                    score = scores[i] + topk_probs[j]\n",
    "                    all_candidates.append((candidate, score))\n",
    "\n",
    "            all_candidates = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "            sequences = [cand[0] for cand in all_candidates]\n",
    "            scores = torch.tensor([cand[1] for cand in all_candidates], device=device)\n",
    "\n",
    "        for i, seq in enumerate(sequences):\n",
    "            length_penalty = (len(seq)) ** length_penalty_alpha\n",
    "            completed.append((seq, scores[i].item() / length_penalty))\n",
    "\n",
    "        completed = sorted(completed, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        best_seq = completed[0][0]\n",
    "        return [idx2token[idx] for idx in best_seq[1:-1] if idx in idx2token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80043422",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model_with_beam(model, dataset, token2idx, idx2token, num_samples, beam_width=3, max_len=256, visualize=True):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    def tokens_to_str(tokens):\n",
    "        return ''.join(t for t in tokens if t not in ['<s>', '</s>', '<pad>'])\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        image, label = dataset[i]\n",
    "        image = image.to(device)\n",
    "        \n",
    "        pred_tokens = beam_search_decode(model, image, token2idx, idx2token, beam_width, max_len, device=device, length_penalty_alpha=0.6)\n",
    "        true_tokens = [idx2token[idx.item()] for idx in label if idx.item() != token2idx['<pad>']]\n",
    "\n",
    "        pred_str = tokens_to_str(pred_tokens)\n",
    "        true_str = tokens_to_str(true_tokens)\n",
    "\n",
    "        match = pred_str == true_str\n",
    "        correct += int(match)\n",
    "        total += 1\n",
    "\n",
    "        if visualize:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 3))\n",
    "            axes[0].imshow(image.permute(1, 2, 0).squeeze().cpu(), cmap='gray')\n",
    "            axes[0].set_title(\"Input Image\")\n",
    "            axes[0].axis(\"off\")\n",
    "\n",
    "            axes[1].text(0.5, 0.7, f\"GT: ${true_str}$\", ha='center', fontsize=12)\n",
    "            axes[1].text(0.5, 0.3, f\"Pred: ${pred_str}$\", ha='center', fontsize=12, color=('green' if match else 'red'))\n",
    "            axes[1].axis(\"off\")\n",
    "            plt.show()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Processed {i+1}/{num_samples} samples...\")\n",
    "\n",
    "    acc = correct / total\n",
    "    print(f\"\\nBeam Search Accuracy (exact match): {acc * 100:.2f}%\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_with_beam(\n",
    "    model=model,\n",
    "    dataset=test_dataset,\n",
    "    token2idx=token2idx,\n",
    "    idx2token=idx2token,\n",
    "    num_samples=len(test_dataset),\n",
    "    beam_width=3,\n",
    "    visualize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6d64c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_token_level_accuracy(model, dataset, token2idx, idx2token, num_samples=50, beam_width=3, max_len=256):\n",
    "    model.eval()\n",
    "    total_tokens = 0\n",
    "    correct_tokens = 0\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    print(f\"Evaluating on device: {device}\")\n",
    "\n",
    "    def strip_tokens(tokens):\n",
    "        return [t for t in tokens if t not in ['<pad>', '<s>', '</s>']]\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        image, label = dataset[i]\n",
    "        image = image.to(device)\n",
    "\n",
    "        pred_tokens = beam_search_decode(model, image, token2idx, idx2token, beam_width, max_len, device=device, length_penalty_alpha=0.6)\n",
    "        true_tokens = [idx2token[idx.item()] for idx in label]\n",
    "\n",
    "        pred = strip_tokens(pred_tokens)\n",
    "        true = strip_tokens(true_tokens)\n",
    "\n",
    "        for p, t in zip(pred, true):\n",
    "            total_tokens += 1\n",
    "            if p == t:\n",
    "                correct_tokens += 1\n",
    "\n",
    "        total_tokens += abs(len(pred) - len(true))\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Processed {i+1}/{num_samples} samples...\")\n",
    "\n",
    "    accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0.0\n",
    "    print(f\"Token-level accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21696c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate_token_level_accuracy(\n",
    "    model=model,\n",
    "    dataset=test_dataset,\n",
    "    token2idx=token2idx,\n",
    "    idx2token=idx2token,\n",
    "    num_samples=len(test_dataset),\n",
    "    beam_width=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a8c050",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_edit_distance(model, dataset, token2idx, idx2token, num_samples=50, beam_width=3, max_len=256):\n",
    "    model.eval()\n",
    "    total_ned = 0.0\n",
    "    count = 0\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    def strip_tokens(tokens):\n",
    "        return [t for t in tokens if t not in ['<pad>', '<s>', '</s>']]\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        image, label = dataset[i]\n",
    "        image = image.to(device)\n",
    "\n",
    "        pred_tokens = beam_search_decode(model, image, token2idx, idx2token, beam_width, max_len, device=device, length_penalty_alpha=0.6)\n",
    "        true_tokens = [idx2token[idx.item()] for idx in label]\n",
    "\n",
    "        pred = strip_tokens(pred_tokens)\n",
    "        true = strip_tokens(true_tokens)\n",
    "\n",
    "        pred_str = ' '.join(pred)\n",
    "        true_str = ' '.join(true)\n",
    "\n",
    "        if len(true_str.strip()) == 0:\n",
    "            continue\n",
    "\n",
    "        lev_dist = Levenshtein.distance(pred_str, true_str)\n",
    "        max_len_str = max(len(pred_str), len(true_str))\n",
    "        ned = 1 - lev_dist / max_len_str\n",
    "        total_ned += ned\n",
    "        count += 1\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Processed {i+1}/{num_samples} samples...\")\n",
    "\n",
    "    avg_ned = total_ned / count if count > 0 else 0.0\n",
    "    print(f\"Normalized Edit Distance (NED): {avg_ned * 100:.2f}%\")\n",
    "    return avg_ned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7589448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_edit_distance(\n",
    "    model=model,\n",
    "    dataset=test_dataset,\n",
    "    token2idx=token2idx,\n",
    "    idx2token=idx2token,\n",
    "    num_samples=len(test_dataset),\n",
    "    beam_width=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3fe03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def export_predictions_to_csv(model, dataset, token2idx, idx2token, beam_width=3, max_len=256, num_samples=100, output_file=\"prediction_results.csv\"):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    def strip_tokens(tokens):\n",
    "        return [t for t in tokens if t not in ['<pad>', '<s>', '</s>']]\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        image, label = dataset[i]\n",
    "        image = image.to(device)\n",
    "\n",
    "        pred_tokens = beam_search_decode(model, image, token2idx, idx2token, beam_width, max_len, device=device)\n",
    "        true_tokens = [idx2token[idx.item()] for idx in label]\n",
    "\n",
    "        pred = strip_tokens(pred_tokens)\n",
    "        true = strip_tokens(true_tokens)\n",
    "\n",
    "        pred_str = ' '.join(pred)\n",
    "        true_str = ' '.join(true)\n",
    "\n",
    "        sentence_match = pred == true\n",
    "\n",
    "        if len(true) > 0:\n",
    "            match_count = sum(p == t for p, t in zip(pred, true))\n",
    "            token_accuracy = match_count / len(true)\n",
    "        else:\n",
    "            token_accuracy = 0.0\n",
    "\n",
    "        if len(true_str.strip()) > 0:\n",
    "            lev = Levenshtein.distance(pred_str, true_str)\n",
    "            ned = 1 - lev / max(len(pred_str), len(true_str))\n",
    "        else:\n",
    "            ned = 0.0\n",
    "\n",
    "        results.append({\n",
    "            \"index\": i,\n",
    "            \"true_latex\": true_str,\n",
    "            \"pred_latex\": pred_str,\n",
    "            \"sentence_match\": sentence_match,\n",
    "            \"token_accuracy(%)\": round(token_accuracy * 100, 2),\n",
    "            \"normalized_edit_distance(%)\": round(ned * 100, 2)\n",
    "        })\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Processed {i+1}/{num_samples} samples...\")\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved prediction results to: {output_file}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d25204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = export_predictions_to_csv(\n",
    "    model=model,\n",
    "    dataset=test_dataset,\n",
    "    token2idx=token2idx,\n",
    "    idx2token=idx2token,\n",
    "    beam_width=3,\n",
    "    num_samples=len(test_dataset),\n",
    "    output_file=\"latex_predictions.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aef2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_prediction_comparison(image_tensor, true_latex, pred_latex):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    gs = gridspec.GridSpec(1, 3, width_ratios=[1, 1, 1])\n",
    "\n",
    "    # images\n",
    "    ax0 = plt.subplot(gs[0])\n",
    "    ax0.imshow(image_tensor.permute(1, 2, 0).squeeze(), cmap='gray')\n",
    "    ax0.set_title(\"Input Image\")\n",
    "    ax0.axis(\"off\")\n",
    "\n",
    "    # GT LaTeX\n",
    "    ax1 = plt.subplot(gs[1])\n",
    "    ax1.text(0.5, 0.5, f\"${true_latex}$\", fontsize=18, ha='center', va='center')\n",
    "    ax1.set_title(\"Ground Truth\")\n",
    "    ax1.axis(\"off\")\n",
    "\n",
    "    # Prediction LaTeX\n",
    "    ax2 = plt.subplot(gs[2])\n",
    "    ax2.text(0.5, 0.5, f\"${pred_latex}$\", fontsize=18, ha='center', va='center')\n",
    "    ax2.set_title(\"Prediction\")\n",
    "    ax2.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6889bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = random.randint(0, len(test_dataset) - 1)\n",
    "image, label = test_dataset[index]\n",
    "\n",
    "pred_tokens = beam_search_decode(model, image, token2idx, idx2token, beam_width=3, device=\"cuda\", length_penalty_alpha=0.6)\n",
    "true_tokens = [idx2token[idx.item()] for idx in label]\n",
    "\n",
    "def clean(toks): return ''.join(t for t in toks if t not in ['<s>', '</s>', '<pad>'])\n",
    "true_str = clean(true_tokens)\n",
    "pred_str = clean(pred_tokens)\n",
    "\n",
    "render_prediction_comparison(image, true_str, pred_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280cee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def build_confusion_matrix_gpu(model, dataset, token2idx, idx2token,\n",
    "                                num_samples=200, beam_width=3, max_len=256, top_k_tokens=30):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    def strip(tokens):\n",
    "        return [t for t in tokens if t not in ['<pad>', '<s>', '</s>']]\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        image, label = dataset[i]\n",
    "        image = image.to(device)\n",
    "\n",
    "        pred_tokens = beam_search_decode(model, image, token2idx, idx2token, beam_width, max_len, device=device, length_penalty_alpha=0.6)\n",
    "        true_tokens = [idx2token[idx.item()] for idx in label]\n",
    "\n",
    "        pred = strip(pred_tokens)\n",
    "        true = strip(true_tokens)\n",
    "\n",
    "        min_len = min(len(pred), len(true))\n",
    "        true_labels.extend(true[:min_len])\n",
    "        pred_labels.extend(pred[:min_len])\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Processed {i+1}/{num_samples} samples...\")\n",
    "\n",
    "    all_tokens = Counter(true_labels + pred_labels)\n",
    "    most_common = [tok for tok, _ in all_tokens.most_common(top_k_tokens)]\n",
    "    most_common_set = set(most_common)\n",
    "\n",
    "    def map_tok(tok):\n",
    "        return tok if tok in most_common_set else 'other'\n",
    "\n",
    "    mapped_true = [map_tok(tok) for tok in true_labels]\n",
    "    mapped_pred = [map_tok(tok) for tok in pred_labels]\n",
    "\n",
    "    labels_sorted = sorted(set(mapped_true + mapped_pred))\n",
    "\n",
    "    cm = confusion_matrix(mapped_true, mapped_pred, labels=labels_sorted)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=labels_sorted, yticklabels=labels_sorted)\n",
    "    plt.xlabel(\"Predicted Token\")\n",
    "    plt.ylabel(\"Ground Truth Token\")\n",
    "    plt.title(f\"Confusion Matrix (Top {top_k_tokens} Tokens + 'other')\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return cm, labels_sorted, true_labels, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4388c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_top_confusions(true_labels, pred_labels, top_k=10):\n",
    "    assert len(true_labels) == len(pred_labels)\n",
    "\n",
    "    confusions = Counter()\n",
    "\n",
    "    for t, p in zip(true_labels, pred_labels):\n",
    "        if t != p:\n",
    "            confusions[(t, p)] += 1\n",
    "\n",
    "    most_common_confusions = confusions.most_common(top_k)\n",
    "\n",
    "    print(f\"\\nTop-{top_k} Most Common Confusions:\")\n",
    "    for (true_tok, pred_tok), count in most_common_confusions:\n",
    "        print(f\"  '{true_tok}'  '{pred_tok}' : {count} times\")\n",
    "\n",
    "    return most_common_confusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65041084",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm, labels_sorted, true_labels, pred_labels = build_confusion_matrix_gpu(\n",
    "    model=model,\n",
    "    dataset=test_dataset,\n",
    "    token2idx=token2idx,\n",
    "    idx2token=idx2token,\n",
    "    num_samples=len(test_dataset),\n",
    "    beam_width=3,\n",
    "    max_len=256,\n",
    "    top_k_tokens=30\n",
    ")\n",
    "\n",
    "top_confusions = analyze_top_confusions(true_labels, pred_labels, top_k=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac573797",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_bleu_score(model, dataset, token2idx, idx2token,\n",
    "                        num_samples=100, beam_width=3, max_len=256):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    bleu_scores = []\n",
    "\n",
    "    smoothing_fn = SmoothingFunction().method4\n",
    "\n",
    "    def strip_tokens(tokens):\n",
    "        return [t for t in tokens if t not in ['<pad>', '<s>', '</s>']]\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        image, label = dataset[i]\n",
    "        image = image.to(device)\n",
    "\n",
    "        pred_tokens = beam_search_decode(model, image, token2idx, idx2token,\n",
    "                                         beam_width=beam_width, max_len=max_len,\n",
    "                                         device=device, length_penalty_alpha=0.6)\n",
    "        true_tokens = [idx2token[idx.item()] for idx in label]\n",
    "\n",
    "        pred = strip_tokens(pred_tokens)\n",
    "        true = strip_tokens(true_tokens)\n",
    "\n",
    "        if len(true) == 0:\n",
    "            continue\n",
    "\n",
    "        bleu = sentence_bleu(\n",
    "            [true], pred,\n",
    "            weights=(0.25, 0.25, 0.25, 0.25),\n",
    "            smoothing_function=smoothing_fn\n",
    "        )\n",
    "        bleu_scores.append(bleu)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Processed {i+1}/{num_samples} samples...\")\n",
    "\n",
    "    avg_bleu = np.mean(bleu_scores) if bleu_scores else 0.0\n",
    "    print(f\"\\nAverage BLEU Score: {avg_bleu * 100:.2f}%\")\n",
    "    return avg_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9cc763",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score = evaluate_bleu_score(\n",
    "    model=model,\n",
    "    dataset=test_dataset,\n",
    "    token2idx=token2idx,\n",
    "    idx2token=idx2token,\n",
    "    num_samples=len(test_dataset),\n",
    "    beam_width=3,\n",
    "    max_len=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52509541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "images, labels, _ = next(iter(train_loader))\n",
    "images = images.cuda()\n",
    "\n",
    "tgt_input = labels[:, :-1].cuda()\n",
    "tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).cuda()\n",
    "\n",
    "logits = model(images, tgt_input, tgt_mask)\n",
    "\n",
    "dot = make_dot(logits, params=dict(model.named_parameters()))\n",
    "dot.format = 'png'\n",
    "dot.directory = \"model_graph\"\n",
    "dot.view(\"im2latex_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8fbd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(\n",
    "    model,\n",
    "    input_data=(images, tgt_input, tgt_mask),\n",
    "    depth=3,\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\"],\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
